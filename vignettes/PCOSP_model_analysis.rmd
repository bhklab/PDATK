---
title: "PCOSP: Pancreatic Cancer Overall Survival Predictor"
author:
- name: Vandana Sandhu
- name: Heewon Seo
- name: Christopher Eeles
  affiliation:
  - &pm Bioinformatics and Computational Genomics Laboratory, Princess Margaret Cancer Center,University Health Network, Toronto, Ontario, Canada
  email: christopher.eeles@uhnresearch.ca
- name: Benjamin Haibe-Kains
  affiliation:
  - *pm
  - &mbp Department of Medical Biophysics, University of Toronto, Toronto, Canada
  email: benjamin.haibe.kains@utoronto.ca
date: 2021-02-01
output:
    BiocStyle::html_document
vignette: >
    %\VignetteIndexEntry{PCOSP: Pancreatic Cancer Overall Survival Predictor}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteEncoding{UTF-8}
---

# Pancreate Cancer Overall Survival Predictor

As an example of the utility of the PDATK package, we will provide code
replicating the analysis published in <insert PCSOP citation>. While the
code presented here is run on dummy data to ensure that PDATK is not too large,
the full data from that study can be downloaded from <where to download it?>.

```{r load_dependencies, message=FALSE, warning=FALSE, include=FALSE}
library(PDATK)
library(msigdbr)
```

```{r load_sample_data}
data(sampleCohortList)
sampleCohortList
```

## Split Training and Validation Data

To get started using PDATK, we recommend you place your patient cohorts into
`SurvivalExperiment` objects and then assemble those into a master `CohortList`,
which holds the training and validation data for use with the various
`SurvivalModel`s in this package.


```{r subset_and_split_data}
commonGenes <- findCommonGenes(sampleCohortList)
# Subsets all list items, with subset for specifying rows and select for
# specifying columns
cohortList <- subset(sampleCohortList, subset=commonGenes)

ICGCcohortList <- cohortList[grepl('ICGC', names(cohortList), ignore.case=TRUE)]
validationCohortList <- cohortList[!grepl('icgc', names(cohortList),
    ignore.case=TRUE)]
```

Since we are interested in predicting survival, it is necessary to remove
patients with insufficient data to be useful. In general, we want to remove
patients who did not have an event in our period of interest. As such we remove
samples who dropped out of a study, but did not pass away before the first year.

```{r drop_not_censored_patients}
validationCohortList <- dropNotCensored(validationCohortList)
ICGCcohortList <- dropNotCensored(ICGCcohortList)
```
We have now split our data into training and validation data. For this analysis
we will be training using the ICGC cohorts, which includes one cohort with
RNA micro-array data and another with RNA sequencing data. When using multiple
cohorts to train a model, it is required that those cohorts share samples. As
a result we will take as training data all patients shared between the two
cohorts and leave the remainder of patients as part of our validationData.

```{r split_train_test}
# find common samples between our training cohort in a cohort list
commonSamples <- findCommonSamples(ICGCcohortList)

# split into shared samples for training, the rest for testing
ICGCtrainCohorts <- subset(ICGCcohortList, select=commonSamples)
ICGCtestCohorts <- subset(ICGCcohortList, select=commonSamples, invert=TRUE)

# merge our training cohort test data into the rest of the validation data
validationCohortList <- c(ICGCtestCohorts, validationCohortList)
```

## Setup A `PCOSP` Model Object

We now have patient molecular data, annotated with the number of days survived
since treatment and the survival status and are ready to apply a `SurvivalModel`
to this data. In this example, we are applying a Pancreatic Cancer Overall
Survival Model, as described in <PCOSP_paper reference>. This object uses the
`switchBox` package to create an ensembl of binary classifiers, whos votes are
then tallied into a PCOSP score. A PCOSP score is simply the proportion of
models predicting good survival out of the total number of models in the
ensemble.

```{r build_PCOSP_model}
PCOSPmodel <- PCOSP(ICGCtrainCohorts, minDaysSurvived=365, randomSeed=1987)

# view the model parameters; these make your model run reproducible
metadata(PCOSPmodel)$modelParams
```
## Training a PCOSP Model

To simplify working with different `SurvivalModel`, we have implemented a
standard workflow that is valid for all `SurvivalModel`s. This involves first
training the model, then using it to predict risk/risk-classes for a set
of validation cohorts and finally assessing performance on the validation data.

To train a model, the `trainModel` method is used. This function abstracts away
the implementation of model training, allowing end-users to focus on applying
the `SurvivalModel` to make predictions without a need to understand the model
internals. We hope this will make the package useful for those unfamiliar or
uninterested in the details of survival prediction methods.

For training a PCOSP model there are two parameters. First, `numModels` is the
number of models to train for use in the ensemble classifier to predict PCOSP
scores. To keep computation brief, we are only training 100 models. However, it
is recommended to use a minimum of 1000 for real world applications. The second
parameter is `minAccuracy`, which is the minimum model accuracy for a trained
model to included in the final model ensemble. Paradoxically, increasing this
too high can actually decrease the overall performance of the PCOSP model; we
recommend 0.6 as a sweet spot between random chance and over-fitting.

```{r}
trainedPCOSPmodel <- trainModel(PCOSPmodel, numModels=100, minAccuracy=0.6)

metadata(trainedPCOSPmodel)$modelParams
```
We can see that after training, the additional model parameters are added to the
`modelParams` item in the model `metadata`. The goal is to ensure that your
model training, prediction and validation are fully reproducible by capturing
the parameters relevant to a specific model.

## Risk Prediction with a PCOSP Model

After training, a model can now be used with new data to make risk predicitons
and classify samples into 'good' or 'bad' survival groups. To do this, the
standard `predictClasses` method is used. Similar to `trainData`, we have
abstracted away the implementation details to provide users with a simple,
consistent interface to the use of `SurvivalModel` objects for making
predictions.

```{r PCOSP_predictions}
PCOSPpredValCohorts <- predictClasses(validationCohortList,
    model=trainedPCOSPmodel)
```

The returned `CohortList` object now indicates that each of the cohorts have
predictions. This information is available in the `elementMetadata` slot of
the cohort list and can be accessed with the `mcols` function from `S4Vectors`.

```{r predicted_elementMetadata}
mcols(PCOSPpredValCohorts)
```
Predicting classes with a specific model adds a corresponding metadata
column to the object `colData`. In the case of a `PCOSP` model, the new column
is called `PCOSP_prob_good` and represents the proportion of models in the
ensemble which predicted good survival for a given sample.

```{r risk_column}
knitr::kable(head(colData(PCOSPpredValCohorts[[1]])))
```

Additionally, binary predictions of good or bad survival can be found in the
`PCOSPpredictions` item of each `SurvivalExperiment`s `metadata`. This contains
the raw predictions from the model for each classifier in the ensemble, ordered
by classifier accuracy. This data is not important for end users, but is used
internally when calculating validation statistics for the model. For users
wishing to classify samples rather than estimate risks, we recommend a
PCOSP cut-off of > 0.5 for good survival prognosis.

```{r raw_predictions}
knitr::kable(metadata(PCOSPpredValCohorts[[1]])$PCOSPpredictions[1:3, 1:5])
```

## Validating A PCOSP Model

The final step in the standard `SurvivalModel` work-flow is to compute performance
statistics for the model on the validation data. This can be accomplished using
the `validateModel` method, which will add statistics to the `validationStats`
slot of a `SurvivalModel` object and the data to the `validationData` slot.

```{r validate_PCOSP_model}
validatedPCOSPmodel <- validateModel(trainedPCOSPmodel,
    valData=PCOSPpredValCohorts[-2]) # drop ICGCSEQ from the validation data
```

```{r validationStats}
knitr::kable(head(validationStats(validatedPCOSPmodel)))
```

Examining the `data.table` from the `validationStats` slot we can see that three
model performance statistics have been calculated for all of the validation
cohorts. Additionally, aggregate statistics have been calculated by molecular
data type and for all cohorts combined. This table can be used to generate
model performance plots. We have included several functions for examining
model performance.

## Plotting Model Performance


```{r PCOSP_D_index_forestplot}
PCOSPdIndexForestPlot <- forestPlot(validatedPCOSPmodel, stat='D_index',
    transform='log2')
PCOSPdIndexForestPlot
```



```{r PCOSP_concordance_index_forestplot}
PCOSPconcIndexForestPlot <- forestPlot(validatedPCOSPmodel, stat='concordance_index')
PCOSPconcIndexForestPlot
```


```{r PCOSP_ROC_curve, fig.height=8, fig.width=8, message=FALSE}
cohortROCplots <- plotROC(validatedPCOSPmodel, alpha=0.05)
cohortROCplots
```
# Permutations Testing for a PCOSP Model

To demonstrate the performance of a PCOSP model to random chance, we have
included two models which permute the patient prognosis labels and the gene
names. These models can be used to evaluate if you model does better than
random chance.

## Random Label Shuffling Model

The `RLSModel` class is a `SurvivalModel` using the same risk prediction
algorithm as a `PCOSP` model, but randomizing the patient prognosis labels
in each of the individual KTSP classifiers used in the classification ensemble.
Given this random shuffling, we expect the classification results for this model
to be no better than random chance.

The work-flow for this model follows the standard `SurvivalModel` work-flow.

### Construct the Model Object

```{r RLSModeL_constructor}

### Train the Model

```{r RLSModel_training}
```

### Predict the Classes

```{r }
```

### Validate Model Performance

```{r }
```



